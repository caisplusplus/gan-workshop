{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# All the necesssary imports.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.core import Activation, Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import initializers\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in the data.\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Normalize to [-1, 1] (easier to work with)\n",
    "x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "# For the sake of time don't work with the entire dataset.\n",
    "x_train = x_train[:10000]\n",
    "y_train = x_train[:10000]\n",
    "\n",
    "# Flatten the data.\n",
    "x_train = x_train.reshape((-1, 784))\n",
    "x_test = x_test.reshape((-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b541a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot out a sample image (reshaped to 28 x 28)\n",
    "plt.imshow(x_train[0].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How many noise dimensions our generator should take in\n",
    "NOISE_DIM = 100 # Feel free to tweak this and see what changes\n",
    "\n",
    "# Define the generator.\n",
    "# The generator should have the following layers.\n",
    "# Fully connected from # noise dimensions to 256\n",
    "# LeakyReLU (alpha = 0.2) (what is leaky relu? https://cdn-images-1.medium.com/max/1600/1*A_Bzn0CjUgOXtPCJKnKLqA.jpeg)\n",
    "# Fully connected from 256 to 512\n",
    "# LeakyReLU (alpha = 0.2)\n",
    "# Fully connected from 512 to 1024\n",
    "# LeakyReLU (alpha = 0.2)\n",
    "# Fully connected from 1024 to 784 (28*28)\n",
    "# tanh (To normalize our output from -1 to 1)\n",
    "\n",
    "def generator():\n",
    "    ###################################\n",
    "    #TODO: Implement\n",
    "    model = Sequential()\n",
    "    \n",
    "    # How to add leaky relu\n",
    "    #model.add(LeakyReLU())\n",
    "    # (We'll give you this first layer. Keep the initializer as is -- it works better than the default)\n",
    "    # (You can also keep initializers default for the rest of the layers)\n",
    "    model.add(Dense(256, input_dim=NOISE_DIM, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    \n",
    "    # Add the rest of the layers\n",
    "    \n",
    "    return model\n",
    "    ###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the discriminator here.\n",
    "# Should have the following layers\n",
    "# Fully connected from 784 (28*28) to 1024\n",
    "# LeakyReLU (alpha = 0.2)\n",
    "# Dropout with p = 0.3\n",
    "# Fully connected from 1024 to 512\n",
    "# LeakyReLU (alpha = 0.2)\n",
    "# Dropout with p = 0.3\n",
    "# Fully connected from 512 to 256\n",
    "# LeakyReLU (alpha = 0.2)\n",
    "# Dropout with p = 0.3\n",
    "# Fully connected from 256 to 1\n",
    "# sigmoid (to get our probability)\n",
    "\n",
    "def discriminator():\n",
    "    # (We'll give you this first layer. Keep the initializer as is -- it works better than the default)\n",
    "    model.add(Dense(1024, input_dim=784, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    \n",
    "    # Add the rest of the layers\n",
    "    \n",
    "    return model\n",
    "    ###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feed the generated image into the discriminator. \n",
    "# Weâ€™ll use this to train our generator end-to-end later on, \n",
    "    # while leaving the discriminator weights untouched \n",
    "# Hint: we can add existing Sequential() models to new models, just like we can with any other layer.\n",
    "    # The parameters are then \"passed by reference,\" so that they use the same internal weights.\n",
    "\n",
    "# Remember that we are not updating our discriminator in this step. \n",
    "    # (We'll train the discriminator separately.)\n",
    "# Please refer to \n",
    "    # https://keras.io/getting-started/faq/#how-can-i-freeze-keras-layers\n",
    "    # on how to \"freeze\" or set layers to be untrainable in keras.\n",
    "# We can call this \"freeze\" operation on an entire model.\n",
    "\n",
    "def combine(generator, discriminator):\n",
    "    ###################################\n",
    "    # TODO: Implement\n",
    "    model = Sequential()\n",
    "    return model\n",
    "    ###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the discriminator, generator, and full GAN. \n",
    "\n",
    "# Use this optimizer for each of the models\n",
    "opt = Adam(lr=.0002, beta_1=0.5)\n",
    "\n",
    "#############################################\n",
    "# TODO: Compile generator and discriminator\n",
    "\n",
    "# Discriminator\n",
    "# d = None\n",
    "\n",
    "# Full gan\n",
    "# gd = None\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to display sample from the network\n",
    "def disp_sample(g):\n",
    "    noise = np.random.uniform(-1, 1, size=(batch_size, NOISE_DIM))\n",
    "    generated_images = g.predict(noise, verbose=0)\n",
    "    show_im = generated_images[0]\n",
    "    show_im = (show_im + 1) / 2.0\n",
    "    show_im = show_im.reshape(28, 28)\n",
    "    plt.imshow(show_im, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep this. \n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(100):\n",
    "    print('Epoch #%d' % epoch)\n",
    "    \n",
    "    # Generate an image and display it.\n",
    "    disp_sample(g)\n",
    "    \n",
    "    num_batches = int(x_train.shape[0] / batch_size)\n",
    "    all_g_loss = []\n",
    "    all_d_loss = []\n",
    "    for i in range(num_batches):\n",
    "        #######################################################\n",
    "        #TODO: Complete a training iteration\n",
    "        # Generate noise.\n",
    "        noise = np.random.uniform(-1, 1, size=(batch_size, NOISE_DIM))\n",
    "        \n",
    "        # Generate images from the noise using the generator.\n",
    "            # See 'predict()': https://keras.io/models/sequential/\n",
    "        #generated_images = None\n",
    "        \n",
    "        # Grab the image batch for this iteration. \n",
    "        real_images = x_train[i * batch_size: (i+1) * batch_size]\n",
    "        \n",
    "        ### Train the discriminator using the generated images and the real images ###\n",
    "        \n",
    "        # Contains the real and fake images. (Concatenate!)\n",
    "            # https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.concatenate.html\n",
    "        # X = None\n",
    "        \n",
    "        # Labels if the sample is real (1) or not real (0). \n",
    "            # Numpy 'ones' function: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ones.html\n",
    "            # 'zeros': https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.zeros.html\n",
    "        # y = None\n",
    "        \n",
    "        # Remember that for this line, the discriminator has to have d.trainable = True!\n",
    "            # Update d.trainable as necessary\n",
    "        # d_loss = d.train_on_batch(X, y)\n",
    "        # Now, set d.trainable to False again to get ready for the end-to-end generator training\n",
    "            # (We want to hold the discriminator constant while we're training the generator. See below.)\n",
    "        \n",
    "        # Generate more noise to feed into the full gan network to train the generative portion. \n",
    "        # noise = None\n",
    "        \n",
    "        # Get the g_loss (fill in the ... part between the parantheses)\n",
    "            # Hint: we want the final output to be 1 (indicating that the discriminator was fooled),\n",
    "            # So the labels should be all 1's. \n",
    "            # Then, since the discriminator weights are fixed,\n",
    "            # the generator weights will have to adjust so that the final outputs will be closer to 1 \n",
    "            # (i.e. are are producing more realistic images).\n",
    "        # g_loss = dg.train_on_batch(...)\n",
    "        \n",
    "        # For getting the averages of each epoch\n",
    "        all_d_loss.append(d_loss)\n",
    "        all_g_loss.append(g_loss)\n",
    "        \n",
    "        #######################################################\n",
    "    print('%i D: %.4f, G: %.4f' % (epoch, np.mean(all_d_loss), np.mean(all_g_loss)))\n",
    "\n",
    "#########################################################################################\n",
    "# Why is my generative loss oscillating???   \n",
    "# Don't worry this is normal as the generator is oscillating between possible solutions\n",
    "#########################################################################################\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
