{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All the necesssary imports.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.core import Activation, Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import initializers\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the generator here.\n",
    "# The generator should have the following layers.\n",
    "# Fully connected from # noise dimensions to 256\n",
    "# LeakyReLU\n",
    "# Fully connected from 256 to 512\n",
    "# LeakyReLU\n",
    "# Fully connected from 512 to 1024\n",
    "# LeakyReLU\n",
    "# Fully connected from 1024 to 784 (28*28)\n",
    "# tanh (To normalize our output from -1 to 1)\n",
    "def generator():\n",
    "    ###################################\n",
    "    #TODO: Implement\n",
    "    model = None\n",
    "    return model\n",
    "    ###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the discriminator here.\n",
    "# Should have the following layers\n",
    "# Fully connected from 784 (28*28) to 1024\n",
    "# LeakyReLU\n",
    "# Dropout with p = 0.3\n",
    "# Fully connected from 1024 to 512\n",
    "# LeakyReLU\n",
    "# Dropout with p = 0.3\n",
    "# Fully connected from 512 to 256\n",
    "# LeakyReLU\n",
    "# Dropout with p = 0.3\n",
    "# Fully connected from 256 to 1\n",
    "# sigmoid (to get our probability)\n",
    "def discriminator():\n",
    "    ###################################\n",
    "    #TODO: Implement\n",
    "    model = None\n",
    "    return model\n",
    "    ###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feed the generated image into the discriminator. \n",
    "# Remember that we are not updating our discriminator in this step. \n",
    "# Please refer to \n",
    "# https://keras.io/getting-started/faq/#how-can-i-freeze-keras-layers\n",
    "# on how to \"freeze\" or set layers to be untrainable in keras.\n",
    "# we can call this operation on an entire model.\n",
    "def combine(generator, discriminator):\n",
    "    ###################################\n",
    "    #TODO: Implement\n",
    "    model = None\n",
    "    return model\n",
    "    ###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in the data.\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Normalize to [-1, 1]\n",
    "x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "# For the sake of time don't work with the entire dataset.\n",
    "x_train = x_train[:10000]\n",
    "y_train = x_train[:10000]\n",
    "\n",
    "# Flatten the data.\n",
    "x_train = x_train.reshape((-1, 784))\n",
    "x_test = x_test.reshape((-1, 784))\n",
    "\n",
    "# Compile the discriminator, generator, and full GAN. \n",
    "# Probably use Adam optimizer...\n",
    "\n",
    "#############################################\n",
    "#TODO: Compile generator and discriminator\n",
    "\n",
    "# Discriminator\n",
    "# d = None\n",
    "\n",
    "# Generator\n",
    "# g = None\n",
    "\n",
    "# Full gan\n",
    "# dg = None\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to sample from the network\n",
    "def disp_sample(g):\n",
    "    noise = np.random.uniform(-1, 1, size=(batch_size, 100))\n",
    "    generated_images = g.predict(noise, verbose=0)\n",
    "    show_im = generated_images[0]\n",
    "    show_im = (show_im + 1) / 2.0\n",
    "    show_im = show_im.reshape(28, 28)\n",
    "    plt.imshow(show_im, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep this. \n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(100):\n",
    "    print('Epoch #%d' % epoch)\n",
    "    \n",
    "    # Generate an image and display it.\n",
    "    disp_sample(g)\n",
    "    \n",
    "    num_batches = int(x_train.shape[0] / batch_size)\n",
    "    print('Number batches %i' % num_batches)\n",
    "    for i in range(num_batches):\n",
    "        #######################################################\n",
    "        #TODO: Complete a training iteration\n",
    "        # Generate noise.\n",
    "        # noise = None\n",
    "        \n",
    "        # Generate images from the noise using the generator.\n",
    "        \n",
    "        # Grab the image batch for this iteration. \n",
    "        #generated_images = None\n",
    "        \n",
    "        # Train the discriminator using the generated images and the real images.\n",
    "        real_images = x_train[i * batch_size: (i+1) * batch_size]\n",
    "        \n",
    "        # Contains the real and fake images.\n",
    "        #X = None\n",
    "        \n",
    "        # Labels if the sample is real (1) or not real (0). \n",
    "        #y = None\n",
    "        \n",
    "        # Remember that the discriminator has to have d.trainable = True!\n",
    "        #d_loss = d.train_on_batch(X, y)\n",
    "        \n",
    "        # Generate more noise to feed into the full gan network to train the generative portion. \n",
    "        # noise = None\n",
    "        \n",
    "        # Get the g_loss (fill in the ... part between the parantheses)\n",
    "        # g_loss = dg.train_on_batch(...)\n",
    "        \n",
    "        print('%i(%i/%i) D: %.4f, G: %.4f' % (epoch, i, num_batches, d_loss, g_loss))\n",
    "        \n",
    "        #######################################################\n",
    "\n",
    "#########################################################################################\n",
    "# Why is my generative loss oscillating???   \n",
    "# Don't worry this is normal as the generator is oscillating between possible solutions\n",
    "#########################################################################################\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
